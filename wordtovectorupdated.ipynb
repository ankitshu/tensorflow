{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'machine', 'demand', 'much', 'as', 'market', 'each', 'purpose', 'it', 'widely', 'success', 'from', 'for', 'perfect', 'according', 'behind', 'an', 'between', 'its', 'understand', 'with', 'time', 'to', 'productive', 'flexible', 'these', 'which', 'can', 'flow', 'major', 'operations', 'but', 'more', 'it’s', 'there', 'make', 'smartphones', 'through', 'will', 'that', 'computation', 'numeric', 'technology', 'production', 'run', 'you', 'on', 'along', 'so', 'has', 'ranging', 'be', 'computations', 'a', 'google’s', 'great', 'open', 'tensorboard', 'could', 'library', 'right', 'program', 'one', 'definition', 'graph', 'the', 'community', 'now', 'data', 'addition', 'simple', 'research', 'comes', 'node', 'your', 'even', 'tasks', 'in', 'nodes', 'was', 'documentation', 'accessed', 'developed', 'brain', 'diagrams', 'general', 'know', 'represents', 'tensorflow', 'is', 'being', 'enthusiast', 'team', 'reason', 'our', 'api', 'represent', 'used', 'of', 'specific', 'form', 'performing', 'source', 'relation', 'ready', 'very', 'by', 'are', 'if', 'nature', 'put', 'learning', 'active', 'edge', 'bag'}\n"
     ]
    }
   ],
   "source": [
    "#corpus_raw = 'He is the king . The king is royal . She is the royal  queen . '\n",
    "corpus_raw = '''According to the documentation tensorflow is an open source library for performing machine learning operations . But the more specific definition of tensorflow is that its an open source library for numeric computation which understand the program in form of data flow diagrams . Even if you make a simple addition program there will be a graph ready for it and that graph can be accessed through the tensorboard api which comes along with tensorflow . Each graph node represents a computations and each edge represent relation between these nodes . Tensorflow was developed by Google’s brain team for machine learning research but now it is being widely used for general purpose machine learning tasks . One of the major reason behind its success is  its flexible nature that it could run even on our smartphones . And right know its community is very much active so if you are a machine learning enthusiast it’s the perfect time to put tensorflow in your bag as it’s very productive and has great demand in the technology market ranging from production to research .'''\n",
    "\n",
    "corpus_raw = corpus_raw.lower()\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.' and word != ',': # because we don't want to treat . as a word\n",
    "        words.append(word)\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "print(words)\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['according', 'to'], ['according', 'the'], ['to', 'according'], ['to', 'the'], ['to', 'documentation'], ['the', 'according'], ['the', 'to'], ['the', 'documentation'], ['the', 'tensorflow'], ['documentation', 'to'], ['documentation', 'the'], ['documentation', 'tensorflow'], ['documentation', 'is'], ['tensorflow', 'the'], ['tensorflow', 'documentation'], ['tensorflow', 'is'], ['tensorflow', 'an'], ['is', 'documentation'], ['is', 'tensorflow'], ['is', 'an'], ['is', 'open'], ['an', 'tensorflow'], ['an', 'is'], ['an', 'open'], ['an', 'source'], ['open', 'is'], ['open', 'an'], ['open', 'source'], ['open', 'library'], ['source', 'an'], ['source', 'open'], ['source', 'library'], ['source', 'for'], ['library', 'open'], ['library', 'source'], ['library', 'for'], ['library', 'performing'], ['for', 'source'], ['for', 'library'], ['for', 'performing'], ['for', 'machine'], ['performing', 'library'], ['performing', 'for'], ['performing', 'machine'], ['performing', 'learning'], ['machine', 'for'], ['machine', 'performing'], ['machine', 'learning'], ['machine', 'operations'], ['learning', 'performing'], ['learning', 'machine'], ['learning', 'operations'], ['operations', 'machine'], ['operations', 'learning'], ['but', 'the'], ['but', 'more'], ['the', 'but'], ['the', 'more'], ['the', 'specific'], ['more', 'but'], ['more', 'the'], ['more', 'specific'], ['more', 'definition'], ['specific', 'the'], ['specific', 'more'], ['specific', 'definition'], ['specific', 'of'], ['definition', 'more'], ['definition', 'specific'], ['definition', 'of'], ['definition', 'tensorflow'], ['of', 'specific'], ['of', 'definition'], ['of', 'tensorflow'], ['of', 'is'], ['tensorflow', 'definition'], ['tensorflow', 'of'], ['tensorflow', 'is'], ['tensorflow', 'that'], ['is', 'of'], ['is', 'tensorflow'], ['is', 'that'], ['is', 'its'], ['that', 'tensorflow'], ['that', 'is'], ['that', 'its'], ['that', 'an'], ['its', 'is'], ['its', 'that'], ['its', 'an'], ['its', 'open'], ['an', 'that'], ['an', 'its'], ['an', 'open'], ['an', 'source'], ['open', 'its'], ['open', 'an'], ['open', 'source'], ['open', 'library'], ['source', 'an'], ['source', 'open'], ['source', 'library'], ['source', 'for'], ['library', 'open'], ['library', 'source'], ['library', 'for'], ['library', 'numeric'], ['for', 'source'], ['for', 'library'], ['for', 'numeric'], ['for', 'computation'], ['numeric', 'library'], ['numeric', 'for'], ['numeric', 'computation'], ['numeric', 'which'], ['computation', 'for'], ['computation', 'numeric'], ['computation', 'which'], ['computation', 'understand'], ['which', 'numeric'], ['which', 'computation'], ['which', 'understand'], ['which', 'the'], ['understand', 'computation'], ['understand', 'which'], ['understand', 'the'], ['understand', 'program'], ['the', 'which'], ['the', 'understand'], ['the', 'program'], ['the', 'in'], ['program', 'understand'], ['program', 'the'], ['program', 'in'], ['program', 'form'], ['in', 'the'], ['in', 'program'], ['in', 'form'], ['in', 'of'], ['form', 'program'], ['form', 'in'], ['form', 'of'], ['form', 'data'], ['of', 'in'], ['of', 'form'], ['of', 'data'], ['of', 'flow'], ['data', 'form'], ['data', 'of'], ['data', 'flow'], ['data', 'diagrams'], ['flow', 'of'], ['flow', 'data'], ['flow', 'diagrams'], ['diagrams', 'data'], ['diagrams', 'flow'], ['even', 'if'], ['even', 'you'], ['if', 'even'], ['if', 'you'], ['if', 'make'], ['you', 'even'], ['you', 'if'], ['you', 'make'], ['you', 'a'], ['make', 'if'], ['make', 'you'], ['make', 'a'], ['make', 'simple'], ['a', 'you'], ['a', 'make'], ['a', 'simple'], ['a', 'addition'], ['simple', 'make'], ['simple', 'a'], ['simple', 'addition'], ['simple', 'program'], ['addition', 'a'], ['addition', 'simple'], ['addition', 'program'], ['addition', 'there'], ['program', 'simple'], ['program', 'addition'], ['program', 'there'], ['program', 'will'], ['there', 'addition'], ['there', 'program'], ['there', 'will'], ['there', 'be'], ['will', 'program'], ['will', 'there'], ['will', 'be'], ['will', 'a'], ['be', 'there'], ['be', 'will'], ['be', 'a'], ['be', 'graph'], ['a', 'will'], ['a', 'be'], ['a', 'graph'], ['a', 'ready'], ['graph', 'be'], ['graph', 'a'], ['graph', 'ready'], ['graph', 'for'], ['ready', 'a'], ['ready', 'graph'], ['ready', 'for'], ['ready', 'it'], ['for', 'graph'], ['for', 'ready'], ['for', 'it'], ['for', 'and'], ['it', 'ready'], ['it', 'for'], ['it', 'and'], ['it', 'that'], ['and', 'for'], ['and', 'it'], ['and', 'that'], ['and', 'graph'], ['that', 'it'], ['that', 'and'], ['that', 'graph'], ['that', 'can'], ['graph', 'and'], ['graph', 'that'], ['graph', 'can'], ['graph', 'be'], ['can', 'that'], ['can', 'graph'], ['can', 'be'], ['can', 'accessed'], ['be', 'graph'], ['be', 'can'], ['be', 'accessed'], ['be', 'through'], ['accessed', 'can'], ['accessed', 'be'], ['accessed', 'through'], ['accessed', 'the'], ['through', 'be'], ['through', 'accessed'], ['through', 'the'], ['through', 'tensorboard'], ['the', 'accessed'], ['the', 'through'], ['the', 'tensorboard'], ['the', 'api'], ['tensorboard', 'through'], ['tensorboard', 'the'], ['tensorboard', 'api'], ['tensorboard', 'which'], ['api', 'the'], ['api', 'tensorboard'], ['api', 'which'], ['api', 'comes'], ['which', 'tensorboard'], ['which', 'api'], ['which', 'comes'], ['which', 'along'], ['comes', 'api'], ['comes', 'which'], ['comes', 'along'], ['comes', 'with'], ['along', 'which'], ['along', 'comes'], ['along', 'with'], ['along', 'tensorflow'], ['with', 'comes'], ['with', 'along'], ['with', 'tensorflow'], ['tensorflow', 'along'], ['tensorflow', 'with'], ['each', 'graph'], ['each', 'node'], ['graph', 'each'], ['graph', 'node'], ['graph', 'represents'], ['node', 'each'], ['node', 'graph'], ['node', 'represents'], ['node', 'a'], ['represents', 'graph'], ['represents', 'node'], ['represents', 'a'], ['represents', 'computations'], ['a', 'node'], ['a', 'represents'], ['a', 'computations'], ['a', 'and'], ['computations', 'represents'], ['computations', 'a'], ['computations', 'and'], ['computations', 'each'], ['and', 'a'], ['and', 'computations'], ['and', 'each'], ['and', 'edge'], ['each', 'computations'], ['each', 'and'], ['each', 'edge'], ['each', 'represent'], ['edge', 'and'], ['edge', 'each'], ['edge', 'represent'], ['edge', 'relation'], ['represent', 'each'], ['represent', 'edge'], ['represent', 'relation'], ['represent', 'between'], ['relation', 'edge'], ['relation', 'represent'], ['relation', 'between'], ['relation', 'these'], ['between', 'represent'], ['between', 'relation'], ['between', 'these'], ['between', 'nodes'], ['these', 'relation'], ['these', 'between'], ['these', 'nodes'], ['nodes', 'between'], ['nodes', 'these'], ['tensorflow', 'was'], ['tensorflow', 'developed'], ['was', 'tensorflow'], ['was', 'developed'], ['was', 'by'], ['developed', 'tensorflow'], ['developed', 'was'], ['developed', 'by'], ['developed', 'google’s'], ['by', 'was'], ['by', 'developed'], ['by', 'google’s'], ['by', 'brain'], ['google’s', 'developed'], ['google’s', 'by'], ['google’s', 'brain'], ['google’s', 'team'], ['brain', 'by'], ['brain', 'google’s'], ['brain', 'team'], ['brain', 'for'], ['team', 'google’s'], ['team', 'brain'], ['team', 'for'], ['team', 'machine'], ['for', 'brain'], ['for', 'team'], ['for', 'machine'], ['for', 'learning'], ['machine', 'team'], ['machine', 'for'], ['machine', 'learning'], ['machine', 'research'], ['learning', 'for'], ['learning', 'machine'], ['learning', 'research'], ['learning', 'but'], ['research', 'machine'], ['research', 'learning'], ['research', 'but'], ['research', 'now'], ['but', 'learning'], ['but', 'research'], ['but', 'now'], ['but', 'it'], ['now', 'research'], ['now', 'but'], ['now', 'it'], ['now', 'is'], ['it', 'but'], ['it', 'now'], ['it', 'is'], ['it', 'being'], ['is', 'now'], ['is', 'it'], ['is', 'being'], ['is', 'widely'], ['being', 'it'], ['being', 'is'], ['being', 'widely'], ['being', 'used'], ['widely', 'is'], ['widely', 'being'], ['widely', 'used'], ['widely', 'for'], ['used', 'being'], ['used', 'widely'], ['used', 'for'], ['used', 'general'], ['for', 'widely'], ['for', 'used'], ['for', 'general'], ['for', 'purpose'], ['general', 'used'], ['general', 'for'], ['general', 'purpose'], ['general', 'machine'], ['purpose', 'for'], ['purpose', 'general'], ['purpose', 'machine'], ['purpose', 'learning'], ['machine', 'general'], ['machine', 'purpose'], ['machine', 'learning'], ['machine', 'tasks'], ['learning', 'purpose'], ['learning', 'machine'], ['learning', 'tasks'], ['tasks', 'machine'], ['tasks', 'learning'], ['one', 'of'], ['one', 'the'], ['of', 'one'], ['of', 'the'], ['of', 'major'], ['the', 'one'], ['the', 'of'], ['the', 'major'], ['the', 'reason'], ['major', 'of'], ['major', 'the'], ['major', 'reason'], ['major', 'behind'], ['reason', 'the'], ['reason', 'major'], ['reason', 'behind'], ['reason', 'its'], ['behind', 'major'], ['behind', 'reason'], ['behind', 'its'], ['behind', 'success'], ['its', 'reason'], ['its', 'behind'], ['its', 'success'], ['its', 'is'], ['success', 'behind'], ['success', 'its'], ['success', 'is'], ['success', 'its'], ['is', 'its'], ['is', 'success'], ['is', 'its'], ['is', 'flexible'], ['its', 'success'], ['its', 'is'], ['its', 'flexible'], ['its', 'nature'], ['flexible', 'is'], ['flexible', 'its'], ['flexible', 'nature'], ['flexible', 'that'], ['nature', 'its'], ['nature', 'flexible'], ['nature', 'that'], ['nature', 'it'], ['that', 'flexible'], ['that', 'nature'], ['that', 'it'], ['that', 'could'], ['it', 'nature'], ['it', 'that'], ['it', 'could'], ['it', 'run'], ['could', 'that'], ['could', 'it'], ['could', 'run'], ['could', 'even'], ['run', 'it'], ['run', 'could'], ['run', 'even'], ['run', 'on'], ['even', 'could'], ['even', 'run'], ['even', 'on'], ['even', 'our'], ['on', 'run'], ['on', 'even'], ['on', 'our'], ['on', 'smartphones'], ['our', 'even'], ['our', 'on'], ['our', 'smartphones'], ['smartphones', 'on'], ['smartphones', 'our'], ['and', 'right'], ['and', 'know'], ['right', 'and'], ['right', 'know'], ['right', 'its'], ['know', 'and'], ['know', 'right'], ['know', 'its'], ['know', 'community'], ['its', 'right'], ['its', 'know'], ['its', 'community'], ['its', 'is'], ['community', 'know'], ['community', 'its'], ['community', 'is'], ['community', 'very'], ['is', 'its'], ['is', 'community'], ['is', 'very'], ['is', 'much'], ['very', 'community'], ['very', 'is'], ['very', 'much'], ['very', 'active'], ['much', 'is'], ['much', 'very'], ['much', 'active'], ['much', 'so'], ['active', 'very'], ['active', 'much'], ['active', 'so'], ['active', 'if'], ['so', 'much'], ['so', 'active'], ['so', 'if'], ['so', 'you'], ['if', 'active'], ['if', 'so'], ['if', 'you'], ['if', 'are'], ['you', 'so'], ['you', 'if'], ['you', 'are'], ['you', 'a'], ['are', 'if'], ['are', 'you'], ['are', 'a'], ['are', 'machine'], ['a', 'you'], ['a', 'are'], ['a', 'machine'], ['a', 'learning'], ['machine', 'are'], ['machine', 'a'], ['machine', 'learning'], ['machine', 'enthusiast'], ['learning', 'a'], ['learning', 'machine'], ['learning', 'enthusiast'], ['learning', 'it’s'], ['enthusiast', 'machine'], ['enthusiast', 'learning'], ['enthusiast', 'it’s'], ['enthusiast', 'the'], ['it’s', 'learning'], ['it’s', 'enthusiast'], ['it’s', 'the'], ['it’s', 'perfect'], ['the', 'enthusiast'], ['the', 'it’s'], ['the', 'perfect'], ['the', 'time'], ['perfect', 'it’s'], ['perfect', 'the'], ['perfect', 'time'], ['perfect', 'to'], ['time', 'the'], ['time', 'perfect'], ['time', 'to'], ['time', 'put'], ['to', 'perfect'], ['to', 'time'], ['to', 'put'], ['to', 'tensorflow'], ['put', 'time'], ['put', 'to'], ['put', 'tensorflow'], ['put', 'in'], ['tensorflow', 'to'], ['tensorflow', 'put'], ['tensorflow', 'in'], ['tensorflow', 'your'], ['in', 'put'], ['in', 'tensorflow'], ['in', 'your'], ['in', 'bag'], ['your', 'tensorflow'], ['your', 'in'], ['your', 'bag'], ['your', 'as'], ['bag', 'in'], ['bag', 'your'], ['bag', 'as'], ['bag', 'it’s'], ['as', 'your'], ['as', 'bag'], ['as', 'it’s'], ['as', 'very'], ['it’s', 'bag'], ['it’s', 'as'], ['it’s', 'very'], ['it’s', 'productive'], ['very', 'as'], ['very', 'it’s'], ['very', 'productive'], ['very', 'and'], ['productive', 'it’s'], ['productive', 'very'], ['productive', 'and'], ['productive', 'has'], ['and', 'very'], ['and', 'productive'], ['and', 'has'], ['and', 'great'], ['has', 'productive'], ['has', 'and'], ['has', 'great'], ['has', 'demand'], ['great', 'and'], ['great', 'has'], ['great', 'demand'], ['great', 'in'], ['demand', 'has'], ['demand', 'great'], ['demand', 'in'], ['demand', 'the'], ['in', 'great'], ['in', 'demand'], ['in', 'the'], ['in', 'technology'], ['the', 'demand'], ['the', 'in'], ['the', 'technology'], ['the', 'market'], ['technology', 'in'], ['technology', 'the'], ['technology', 'market'], ['technology', 'ranging'], ['market', 'the'], ['market', 'technology'], ['market', 'ranging'], ['market', 'from'], ['ranging', 'technology'], ['ranging', 'market'], ['ranging', 'from'], ['ranging', 'production'], ['from', 'market'], ['from', 'ranging'], ['from', 'production'], ['from', 'to'], ['production', 'ranging'], ['production', 'from'], ['production', 'to'], ['production', 'research'], ['to', 'from'], ['to', 'production'], ['to', 'research'], ['research', 'production'], ['research', 'to']]\n",
      "and\n",
      "machine\n",
      "demand\n",
      "much\n",
      "as\n",
      "market\n",
      "each\n",
      "purpose\n",
      "it\n",
      "widely\n",
      "success\n",
      "from\n",
      "for\n",
      "perfect\n",
      "according\n",
      "behind\n",
      "an\n",
      "between\n",
      "its\n",
      "understand\n",
      "with\n",
      "time\n",
      "to\n",
      "productive\n",
      "flexible\n",
      "these\n",
      "which\n",
      "can\n",
      "flow\n",
      "major\n",
      "operations\n",
      "but\n",
      "more\n",
      "it’s\n",
      "there\n",
      "make\n",
      "smartphones\n",
      "through\n",
      "will\n",
      "that\n",
      "computation\n",
      "numeric\n",
      "technology\n",
      "production\n",
      "run\n",
      "you\n",
      "on\n",
      "along\n",
      "so\n",
      "has\n",
      "ranging\n",
      "be\n",
      "computations\n",
      "a\n",
      "google’s\n",
      "great\n",
      "open\n",
      "tensorboard\n",
      "could\n",
      "library\n",
      "right\n",
      "program\n",
      "one\n",
      "definition\n",
      "graph\n",
      "the\n",
      "community\n",
      "now\n",
      "data\n",
      "addition\n",
      "simple\n",
      "research\n",
      "comes\n",
      "node\n",
      "your\n",
      "even\n",
      "tasks\n",
      "in\n",
      "nodes\n",
      "was\n",
      "documentation\n",
      "accessed\n",
      "developed\n",
      "brain\n",
      "diagrams\n",
      "general\n",
      "know\n",
      "represents\n",
      "tensorflow\n",
      "is\n",
      "being\n",
      "enthusiast\n",
      "team\n",
      "reason\n",
      "our\n",
      "api\n",
      "represent\n",
      "used\n",
      "of\n",
      "specific\n",
      "form\n",
      "performing\n",
      "source\n",
      "relation\n",
      "ready\n",
      "very\n",
      "by\n",
      "are\n",
      "if\n",
      "nature\n",
      "put\n",
      "learning\n",
      "active\n",
      "edge\n",
      "bag\n"
     ]
    }
   ],
   "source": [
    "metadata = \"/home/madhusudan/environments/tensorboard/wordtovec/metadata.tsv\"\n",
    "print(data)\n",
    "\n",
    "with open(metadata, 'w') as metadata_file:\n",
    "    for row in words:\n",
    "        #c = np.nonzero(y_train)[1][row]\n",
    "        c = row\n",
    "        print(c)\n",
    "        metadata_file.write('{}\\n'.format(c))\n",
    "        \n",
    "\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "\n",
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  9.22304\n",
      "loss is :  8.94652\n",
      "loss is :  8.70738\n",
      "loss is :  8.49978\n",
      "loss is :  8.31911\n",
      "loss is :  8.16157\n",
      "loss is :  8.024\n",
      "loss is :  7.9037\n",
      "loss is :  7.79835\n",
      "loss is :  7.70594\n",
      "loss is :  7.62473\n",
      "loss is :  7.5532\n",
      "loss is :  7.49004\n",
      "loss is :  7.43408\n",
      "loss is :  7.38434\n",
      "loss is :  7.33995\n",
      "loss is :  7.30014\n",
      "loss is :  7.26428\n",
      "loss is :  7.23182\n",
      "loss is :  7.20225\n",
      "loss is :  7.17519\n",
      "loss is :  7.15028\n",
      "loss is :  7.12721\n",
      "loss is :  7.10574\n",
      "loss is :  7.08564\n",
      "loss is :  7.06672\n",
      "loss is :  7.04884\n",
      "loss is :  7.03185\n",
      "loss is :  7.01563\n",
      "loss is :  7.0001\n",
      "loss is :  6.98517\n",
      "loss is :  6.97077\n",
      "loss is :  6.95683\n",
      "loss is :  6.9433\n",
      "loss is :  6.93015\n",
      "loss is :  6.91732\n",
      "loss is :  6.9048\n",
      "loss is :  6.89254\n",
      "loss is :  6.88053\n",
      "loss is :  6.86875\n",
      "loss is :  6.85717\n",
      "loss is :  6.84578\n",
      "loss is :  6.83456\n",
      "loss is :  6.82352\n",
      "loss is :  6.81262\n",
      "loss is :  6.80187\n",
      "loss is :  6.79126\n",
      "loss is :  6.78077\n",
      "loss is :  6.77041\n",
      "loss is :  6.76017\n",
      "loss is :  6.75003\n",
      "loss is :  6.74\n",
      "loss is :  6.73008\n",
      "loss is :  6.72025\n",
      "loss is :  6.71052\n",
      "loss is :  6.70088\n",
      "loss is :  6.69133\n",
      "loss is :  6.68186\n",
      "loss is :  6.67248\n",
      "loss is :  6.66318\n",
      "loss is :  6.65396\n",
      "loss is :  6.64482\n",
      "loss is :  6.63575\n",
      "loss is :  6.62675\n",
      "loss is :  6.61783\n",
      "loss is :  6.60898\n",
      "loss is :  6.6002\n",
      "loss is :  6.59149\n",
      "loss is :  6.58284\n",
      "loss is :  6.57425\n",
      "loss is :  6.56573\n",
      "loss is :  6.55728\n",
      "loss is :  6.54888\n",
      "loss is :  6.54055\n",
      "loss is :  6.53228\n",
      "loss is :  6.52406\n",
      "loss is :  6.5159\n",
      "loss is :  6.5078\n",
      "loss is :  6.49976\n",
      "loss is :  6.49177\n",
      "loss is :  6.48383\n",
      "loss is :  6.47595\n",
      "loss is :  6.46812\n",
      "loss is :  6.46035\n",
      "loss is :  6.45262\n",
      "loss is :  6.44495\n",
      "loss is :  6.43732\n",
      "loss is :  6.42975\n",
      "loss is :  6.42222\n",
      "loss is :  6.41474\n",
      "loss is :  6.40731\n",
      "loss is :  6.39992\n",
      "loss is :  6.39259\n",
      "loss is :  6.38529\n",
      "loss is :  6.37805\n",
      "loss is :  6.37084\n",
      "loss is :  6.36368\n",
      "loss is :  6.35657\n",
      "loss is :  6.3495\n",
      "loss is :  6.34247\n",
      "loss is :  6.33548\n",
      "loss is :  6.32853\n",
      "loss is :  6.32163\n",
      "loss is :  6.31476\n",
      "loss is :  6.30794\n",
      "loss is :  6.30116\n",
      "loss is :  6.29441\n",
      "loss is :  6.2877\n",
      "loss is :  6.28104\n",
      "loss is :  6.27441\n",
      "loss is :  6.26781\n",
      "loss is :  6.26126\n",
      "loss is :  6.25474\n",
      "loss is :  6.24826\n",
      "loss is :  6.24181\n",
      "loss is :  6.2354\n",
      "loss is :  6.22903\n",
      "loss is :  6.22269\n",
      "loss is :  6.21638\n",
      "loss is :  6.21011\n",
      "loss is :  6.20388\n",
      "loss is :  6.19767\n",
      "loss is :  6.1915\n",
      "loss is :  6.18537\n",
      "loss is :  6.17926\n",
      "loss is :  6.17319\n",
      "loss is :  6.16715\n",
      "loss is :  6.16114\n",
      "loss is :  6.15517\n",
      "loss is :  6.14922\n",
      "loss is :  6.14331\n",
      "loss is :  6.13742\n",
      "loss is :  6.13157\n",
      "loss is :  6.12574\n",
      "loss is :  6.11995\n",
      "loss is :  6.11419\n",
      "loss is :  6.10845\n",
      "loss is :  6.10274\n",
      "loss is :  6.09707\n",
      "loss is :  6.09142\n",
      "loss is :  6.08579\n",
      "loss is :  6.0802\n",
      "loss is :  6.07463\n",
      "loss is :  6.06909\n",
      "loss is :  6.06358\n",
      "loss is :  6.0581\n",
      "loss is :  6.05264\n",
      "loss is :  6.04721\n",
      "loss is :  6.0418\n",
      "loss is :  6.03642\n",
      "loss is :  6.03107\n",
      "loss is :  6.02574\n",
      "loss is :  6.02044\n",
      "loss is :  6.01516\n",
      "loss is :  6.00991\n",
      "loss is :  6.00468\n",
      "loss is :  5.99948\n",
      "loss is :  5.9943\n",
      "loss is :  5.98914\n",
      "loss is :  5.98401\n",
      "loss is :  5.9789\n",
      "loss is :  5.97382\n",
      "loss is :  5.96876\n",
      "loss is :  5.96372\n",
      "loss is :  5.9587\n",
      "loss is :  5.95371\n",
      "loss is :  5.94874\n",
      "loss is :  5.94379\n",
      "loss is :  5.93887\n",
      "loss is :  5.93397\n",
      "loss is :  5.92909\n",
      "loss is :  5.92423\n",
      "loss is :  5.91939\n",
      "loss is :  5.91457\n",
      "loss is :  5.90978\n",
      "loss is :  5.905\n",
      "loss is :  5.90025\n",
      "loss is :  5.89552\n",
      "loss is :  5.8908\n",
      "loss is :  5.88611\n",
      "loss is :  5.88144\n",
      "loss is :  5.87679\n",
      "loss is :  5.87216\n",
      "loss is :  5.86754\n",
      "loss is :  5.86295\n",
      "loss is :  5.85838\n",
      "loss is :  5.85383\n",
      "loss is :  5.84929\n",
      "loss is :  5.84478\n",
      "loss is :  5.84028\n",
      "loss is :  5.8358\n",
      "loss is :  5.83134\n",
      "loss is :  5.8269\n",
      "loss is :  5.82248\n",
      "loss is :  5.81808\n",
      "loss is :  5.81369\n",
      "loss is :  5.80932\n",
      "loss is :  5.80497\n",
      "loss is :  5.80064\n",
      "loss is :  5.79633\n",
      "loss is :  5.79203\n",
      "loss is :  5.78775\n",
      "loss is :  5.78348\n",
      "loss is :  5.77924\n",
      "loss is :  5.77501\n",
      "loss is :  5.7708\n",
      "loss is :  5.76661\n",
      "loss is :  5.76243\n",
      "loss is :  5.75826\n",
      "loss is :  5.75412\n",
      "loss is :  5.74999\n",
      "loss is :  5.74588\n",
      "loss is :  5.74178\n",
      "loss is :  5.7377\n",
      "loss is :  5.73363\n",
      "loss is :  5.72959\n",
      "loss is :  5.72555\n",
      "loss is :  5.72153\n",
      "loss is :  5.71753\n",
      "loss is :  5.71354\n",
      "loss is :  5.70957\n",
      "loss is :  5.70562\n",
      "loss is :  5.70167\n",
      "loss is :  5.69775\n",
      "loss is :  5.69383\n",
      "loss is :  5.68994\n",
      "loss is :  5.68606\n",
      "loss is :  5.68219\n",
      "loss is :  5.67833\n",
      "loss is :  5.6745\n",
      "loss is :  5.67067\n",
      "loss is :  5.66686\n",
      "loss is :  5.66306\n",
      "loss is :  5.65928\n",
      "loss is :  5.65551\n",
      "loss is :  5.65176\n",
      "loss is :  5.64802\n",
      "loss is :  5.64429\n",
      "loss is :  5.64057\n",
      "loss is :  5.63687\n",
      "loss is :  5.63319\n",
      "loss is :  5.62951\n",
      "loss is :  5.62585\n",
      "loss is :  5.62221\n",
      "loss is :  5.61857\n",
      "loss is :  5.61495\n",
      "loss is :  5.61134\n",
      "loss is :  5.60775\n",
      "loss is :  5.60417\n",
      "loss is :  5.6006\n",
      "loss is :  5.59704\n",
      "loss is :  5.59349\n",
      "loss is :  5.58996\n",
      "loss is :  5.58644\n",
      "loss is :  5.58293\n",
      "loss is :  5.57944\n",
      "loss is :  5.57596\n",
      "loss is :  5.57249\n",
      "loss is :  5.56903\n",
      "loss is :  5.56558\n",
      "loss is :  5.56215\n",
      "loss is :  5.55872\n",
      "loss is :  5.55531\n",
      "loss is :  5.55191\n",
      "loss is :  5.54853\n",
      "loss is :  5.54515\n",
      "loss is :  5.54179\n",
      "loss is :  5.53843\n",
      "loss is :  5.53509\n",
      "loss is :  5.53176\n",
      "loss is :  5.52844\n",
      "loss is :  5.52513\n",
      "loss is :  5.52184\n",
      "loss is :  5.51855\n",
      "loss is :  5.51528\n",
      "loss is :  5.51201\n",
      "loss is :  5.50876\n",
      "loss is :  5.50552\n",
      "loss is :  5.50229\n",
      "loss is :  5.49907\n",
      "loss is :  5.49586\n",
      "loss is :  5.49266\n",
      "loss is :  5.48947\n",
      "loss is :  5.48629\n",
      "loss is :  5.48313\n",
      "loss is :  5.47997\n",
      "loss is :  5.47682\n",
      "loss is :  5.47368\n",
      "loss is :  5.47056\n",
      "loss is :  5.46744\n",
      "loss is :  5.46434\n",
      "loss is :  5.46124\n",
      "loss is :  5.45815\n",
      "loss is :  5.45508\n",
      "loss is :  5.45201\n",
      "loss is :  5.44896\n",
      "loss is :  5.44591\n",
      "loss is :  5.44287\n",
      "loss is :  5.43985\n",
      "loss is :  5.43683\n",
      "loss is :  5.43382\n",
      "loss is :  5.43082\n",
      "loss is :  5.42783\n",
      "loss is :  5.42486\n",
      "loss is :  5.42189\n",
      "loss is :  5.41893\n",
      "loss is :  5.41598\n",
      "loss is :  5.41303\n",
      "loss is :  5.4101\n",
      "loss is :  5.40718\n",
      "loss is :  5.40426\n",
      "loss is :  5.40136\n",
      "loss is :  5.39846\n",
      "loss is :  5.39558\n",
      "loss is :  5.3927\n",
      "loss is :  5.38983\n",
      "loss is :  5.38697\n",
      "loss is :  5.38412\n",
      "loss is :  5.38128\n",
      "loss is :  5.37844\n",
      "loss is :  5.37562\n",
      "loss is :  5.3728\n",
      "loss is :  5.36999\n",
      "loss is :  5.36719\n",
      "loss is :  5.3644\n",
      "loss is :  5.36162\n",
      "loss is :  5.35885\n",
      "loss is :  5.35609\n",
      "loss is :  5.35333\n",
      "loss is :  5.35058\n",
      "loss is :  5.34784\n",
      "loss is :  5.34511\n",
      "loss is :  5.34238\n",
      "loss is :  5.33967\n",
      "loss is :  5.33696\n",
      "loss is :  5.33426\n",
      "loss is :  5.33157\n",
      "loss is :  5.32889\n",
      "loss is :  5.32622\n",
      "loss is :  5.32355\n",
      "loss is :  5.32089\n",
      "loss is :  5.31824\n",
      "loss is :  5.3156\n",
      "loss is :  5.31296\n",
      "loss is :  5.31033\n",
      "loss is :  5.30771\n",
      "loss is :  5.3051\n",
      "loss is :  5.3025\n",
      "loss is :  5.2999\n",
      "loss is :  5.29731\n",
      "loss is :  5.29473\n",
      "loss is :  5.29216\n",
      "loss is :  5.28959\n",
      "loss is :  5.28703\n",
      "loss is :  5.28448\n",
      "loss is :  5.28194\n",
      "loss is :  5.2794\n",
      "loss is :  5.27688\n",
      "loss is :  5.27435\n",
      "loss is :  5.27184\n",
      "loss is :  5.26933\n",
      "loss is :  5.26683\n",
      "loss is :  5.26434\n",
      "loss is :  5.26186\n",
      "loss is :  5.25938\n",
      "loss is :  5.25691\n",
      "loss is :  5.25444\n",
      "loss is :  5.25199\n",
      "loss is :  5.24954\n",
      "loss is :  5.24709\n",
      "loss is :  5.24466\n",
      "loss is :  5.24223\n",
      "loss is :  5.23981\n",
      "loss is :  5.23739\n",
      "loss is :  5.23498\n",
      "loss is :  5.23258\n",
      "loss is :  5.23019\n",
      "loss is :  5.2278\n",
      "loss is :  5.22542\n",
      "loss is :  5.22304\n",
      "loss is :  5.22068\n",
      "loss is :  5.21832\n",
      "loss is :  5.21596\n",
      "loss is :  5.21361\n",
      "loss is :  5.21127\n",
      "loss is :  5.20894\n",
      "loss is :  5.20661\n",
      "loss is :  5.20429\n",
      "loss is :  5.20197\n",
      "loss is :  5.19967\n",
      "loss is :  5.19736\n",
      "loss is :  5.19507\n",
      "loss is :  5.19278\n",
      "loss is :  5.1905\n",
      "loss is :  5.18822\n",
      "loss is :  5.18595\n",
      "loss is :  5.18368\n",
      "loss is :  5.18143\n",
      "loss is :  5.17917\n",
      "loss is :  5.17693\n",
      "loss is :  5.17469\n",
      "loss is :  5.17246\n",
      "loss is :  5.17023\n",
      "loss is :  5.16801\n",
      "loss is :  5.16579\n",
      "loss is :  5.16358\n",
      "loss is :  5.16138\n",
      "loss is :  5.15918\n",
      "loss is :  5.15699\n",
      "loss is :  5.15481\n",
      "loss is :  5.15263\n",
      "loss is :  5.15045\n",
      "loss is :  5.14829\n",
      "loss is :  5.14613\n",
      "loss is :  5.14397\n",
      "loss is :  5.14182\n",
      "loss is :  5.13968\n",
      "loss is :  5.13754\n",
      "loss is :  5.13541\n",
      "loss is :  5.13328\n",
      "loss is :  5.13116\n",
      "loss is :  5.12904\n",
      "loss is :  5.12693\n",
      "loss is :  5.12483\n",
      "loss is :  5.12273\n",
      "loss is :  5.12064\n",
      "loss is :  5.11855\n",
      "loss is :  5.11647\n",
      "loss is :  5.11439\n",
      "loss is :  5.11232\n",
      "loss is :  5.11025\n",
      "loss is :  5.10819\n",
      "loss is :  5.10614\n",
      "loss is :  5.10409\n",
      "loss is :  5.10205\n",
      "loss is :  5.10001\n",
      "loss is :  5.09797\n",
      "loss is :  5.09595\n",
      "loss is :  5.09392\n",
      "loss is :  5.09191\n",
      "loss is :  5.0899\n",
      "loss is :  5.08789\n",
      "loss is :  5.08589\n",
      "loss is :  5.08389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  5.0819\n",
      "loss is :  5.07991\n",
      "loss is :  5.07793\n",
      "loss is :  5.07596\n",
      "loss is :  5.07399\n",
      "loss is :  5.07202\n",
      "loss is :  5.07006\n",
      "loss is :  5.06811\n",
      "loss is :  5.06616\n",
      "loss is :  5.06421\n",
      "loss is :  5.06227\n",
      "loss is :  5.06034\n",
      "loss is :  5.05841\n",
      "loss is :  5.05648\n",
      "loss is :  5.05456\n",
      "loss is :  5.05264\n",
      "loss is :  5.05073\n",
      "loss is :  5.04883\n",
      "loss is :  5.04693\n",
      "loss is :  5.04503\n",
      "loss is :  5.04314\n",
      "loss is :  5.04125\n",
      "loss is :  5.03937\n",
      "loss is :  5.03749\n",
      "loss is :  5.03562\n",
      "loss is :  5.03375\n",
      "loss is :  5.03189\n",
      "loss is :  5.03003\n",
      "loss is :  5.02818\n",
      "loss is :  5.02633\n",
      "loss is :  5.02448\n",
      "loss is :  5.02264\n",
      "loss is :  5.02081\n",
      "loss is :  5.01898\n",
      "loss is :  5.01715\n",
      "loss is :  5.01533\n",
      "loss is :  5.01351\n",
      "loss is :  5.0117\n",
      "loss is :  5.00989\n",
      "loss is :  5.00809\n",
      "loss is :  5.00629\n",
      "loss is :  5.0045\n",
      "loss is :  5.00271\n",
      "loss is :  5.00092\n",
      "loss is :  4.99914\n",
      "loss is :  4.99736\n",
      "loss is :  4.99559\n",
      "loss is :  4.99382\n",
      "loss is :  4.99206\n",
      "loss is :  4.9903\n",
      "loss is :  4.98854\n",
      "loss is :  4.98679\n",
      "loss is :  4.98504\n",
      "loss is :  4.9833\n",
      "loss is :  4.98156\n",
      "loss is :  4.97983\n",
      "loss is :  4.9781\n",
      "loss is :  4.97637\n",
      "loss is :  4.97465\n",
      "loss is :  4.97293\n",
      "loss is :  4.97122\n",
      "loss is :  4.96951\n",
      "loss is :  4.96781\n",
      "loss is :  4.9661\n",
      "loss is :  4.96441\n",
      "loss is :  4.96271\n",
      "loss is :  4.96103\n",
      "loss is :  4.95934\n",
      "loss is :  4.95766\n",
      "loss is :  4.95598\n",
      "loss is :  4.95431\n",
      "loss is :  4.95264\n",
      "loss is :  4.95098\n",
      "loss is :  4.94931\n",
      "loss is :  4.94766\n",
      "loss is :  4.946\n",
      "loss is :  4.94436\n",
      "loss is :  4.94271\n",
      "loss is :  4.94107\n",
      "loss is :  4.93943\n",
      "loss is :  4.9378\n",
      "loss is :  4.93617\n",
      "loss is :  4.93454\n",
      "loss is :  4.93292\n",
      "loss is :  4.9313\n",
      "loss is :  4.92968\n",
      "loss is :  4.92807\n",
      "loss is :  4.92647\n",
      "loss is :  4.92486\n",
      "loss is :  4.92326\n",
      "loss is :  4.92167\n",
      "loss is :  4.92007\n",
      "loss is :  4.91848\n",
      "loss is :  4.9169\n",
      "loss is :  4.91532\n",
      "loss is :  4.91374\n",
      "loss is :  4.91216\n",
      "loss is :  4.91059\n",
      "loss is :  4.90903\n",
      "loss is :  4.90746\n",
      "loss is :  4.9059\n",
      "loss is :  4.90435\n",
      "loss is :  4.90279\n",
      "loss is :  4.90124\n",
      "loss is :  4.8997\n",
      "loss is :  4.89816\n",
      "loss is :  4.89662\n",
      "loss is :  4.89508\n",
      "loss is :  4.89355\n",
      "loss is :  4.89202\n",
      "loss is :  4.8905\n",
      "loss is :  4.88898\n",
      "loss is :  4.88746\n",
      "loss is :  4.88594\n",
      "loss is :  4.88443\n",
      "loss is :  4.88292\n",
      "loss is :  4.88142\n",
      "loss is :  4.87992\n",
      "loss is :  4.87842\n",
      "loss is :  4.87692\n",
      "loss is :  4.87543\n",
      "loss is :  4.87395\n",
      "loss is :  4.87246\n",
      "loss is :  4.87098\n",
      "loss is :  4.8695\n",
      "loss is :  4.86803\n",
      "loss is :  4.86656\n",
      "loss is :  4.86509\n",
      "loss is :  4.86362\n",
      "loss is :  4.86216\n",
      "loss is :  4.8607\n",
      "loss is :  4.85925\n",
      "loss is :  4.85779\n",
      "loss is :  4.85634\n",
      "loss is :  4.8549\n",
      "loss is :  4.85346\n",
      "loss is :  4.85202\n",
      "loss is :  4.85058\n",
      "loss is :  4.84915\n",
      "loss is :  4.84772\n",
      "loss is :  4.84629\n",
      "loss is :  4.84486\n",
      "loss is :  4.84344\n",
      "loss is :  4.84202\n",
      "loss is :  4.84061\n",
      "loss is :  4.8392\n",
      "loss is :  4.83779\n",
      "loss is :  4.83638\n",
      "loss is :  4.83498\n",
      "loss is :  4.83358\n",
      "loss is :  4.83218\n",
      "loss is :  4.83079\n",
      "loss is :  4.8294\n",
      "loss is :  4.82801\n",
      "loss is :  4.82663\n",
      "loss is :  4.82524\n",
      "loss is :  4.82386\n",
      "loss is :  4.82249\n",
      "loss is :  4.82112\n",
      "loss is :  4.81974\n",
      "loss is :  4.81838\n",
      "loss is :  4.81701\n",
      "loss is :  4.81565\n",
      "loss is :  4.81429\n",
      "loss is :  4.81294\n",
      "loss is :  4.81158\n",
      "loss is :  4.81023\n",
      "loss is :  4.80888\n",
      "loss is :  4.80754\n",
      "loss is :  4.8062\n",
      "loss is :  4.80486\n",
      "loss is :  4.80352\n",
      "loss is :  4.80219\n",
      "loss is :  4.80086\n",
      "loss is :  4.79953\n",
      "loss is :  4.7982\n",
      "loss is :  4.79688\n",
      "loss is :  4.79556\n",
      "loss is :  4.79424\n",
      "loss is :  4.79293\n",
      "loss is :  4.79162\n",
      "loss is :  4.79031\n",
      "loss is :  4.789\n",
      "loss is :  4.7877\n",
      "loss is :  4.7864\n",
      "loss is :  4.7851\n",
      "loss is :  4.7838\n",
      "loss is :  4.78251\n",
      "loss is :  4.78122\n",
      "loss is :  4.77993\n",
      "loss is :  4.77865\n",
      "loss is :  4.77736\n",
      "loss is :  4.77608\n",
      "loss is :  4.77481\n",
      "loss is :  4.77353\n",
      "loss is :  4.77226\n",
      "loss is :  4.77099\n",
      "loss is :  4.76972\n",
      "loss is :  4.76846\n",
      "loss is :  4.7672\n",
      "loss is :  4.76594\n",
      "loss is :  4.76468\n",
      "loss is :  4.76342\n",
      "loss is :  4.76217\n",
      "loss is :  4.76092\n",
      "loss is :  4.75968\n",
      "loss is :  4.75843\n",
      "loss is :  4.75719\n",
      "loss is :  4.75595\n",
      "loss is :  4.75471\n",
      "loss is :  4.75348\n",
      "loss is :  4.75224\n",
      "loss is :  4.75101\n",
      "loss is :  4.74979\n",
      "loss is :  4.74856\n",
      "loss is :  4.74734\n",
      "loss is :  4.74612\n",
      "loss is :  4.7449\n",
      "loss is :  4.74368\n",
      "loss is :  4.74247\n",
      "loss is :  4.74126\n",
      "loss is :  4.74005\n",
      "loss is :  4.73885\n",
      "loss is :  4.73764\n",
      "loss is :  4.73644\n",
      "loss is :  4.73524\n",
      "loss is :  4.73404\n",
      "loss is :  4.73285\n",
      "loss is :  4.73166\n",
      "loss is :  4.73047\n",
      "loss is :  4.72928\n",
      "loss is :  4.72809\n",
      "loss is :  4.72691\n",
      "loss is :  4.72573\n",
      "loss is :  4.72455\n",
      "loss is :  4.72338\n",
      "loss is :  4.7222\n",
      "loss is :  4.72103\n",
      "loss is :  4.71986\n",
      "loss is :  4.71869\n",
      "loss is :  4.71753\n",
      "loss is :  4.71637\n",
      "loss is :  4.7152\n",
      "loss is :  4.71405\n",
      "loss is :  4.71289\n",
      "loss is :  4.71174\n",
      "loss is :  4.71058\n",
      "loss is :  4.70943\n",
      "loss is :  4.70829\n",
      "loss is :  4.70714\n",
      "loss is :  4.706\n",
      "loss is :  4.70486\n",
      "loss is :  4.70372\n",
      "loss is :  4.70258\n",
      "loss is :  4.70145\n",
      "loss is :  4.70031\n",
      "loss is :  4.69918\n",
      "loss is :  4.69805\n",
      "loss is :  4.69693\n",
      "loss is :  4.6958\n",
      "loss is :  4.69468\n",
      "loss is :  4.69356\n",
      "loss is :  4.69244\n",
      "loss is :  4.69133\n",
      "loss is :  4.69021\n",
      "loss is :  4.6891\n",
      "loss is :  4.68799\n",
      "loss is :  4.68688\n",
      "loss is :  4.68578\n",
      "loss is :  4.68467\n",
      "loss is :  4.68357\n",
      "loss is :  4.68247\n",
      "loss is :  4.68137\n",
      "loss is :  4.68028\n",
      "loss is :  4.67918\n",
      "loss is :  4.67809\n",
      "loss is :  4.677\n",
      "loss is :  4.67592\n",
      "loss is :  4.67483\n",
      "loss is :  4.67375\n",
      "loss is :  4.67266\n",
      "loss is :  4.67158\n",
      "loss is :  4.67051\n",
      "loss is :  4.66943\n",
      "loss is :  4.66835\n",
      "loss is :  4.66728\n",
      "loss is :  4.66621\n",
      "loss is :  4.66514\n",
      "loss is :  4.66408\n",
      "loss is :  4.66301\n",
      "loss is :  4.66195\n",
      "loss is :  4.66089\n",
      "loss is :  4.65983\n",
      "loss is :  4.65877\n",
      "loss is :  4.65772\n",
      "loss is :  4.65667\n",
      "loss is :  4.65561\n",
      "loss is :  4.65456\n",
      "loss is :  4.65352\n",
      "loss is :  4.65247\n",
      "loss is :  4.65143\n",
      "loss is :  4.65038\n",
      "loss is :  4.64935\n",
      "loss is :  4.64831\n",
      "loss is :  4.64727\n",
      "loss is :  4.64623\n",
      "loss is :  4.6452\n",
      "loss is :  4.64417\n",
      "loss is :  4.64314\n",
      "loss is :  4.64211\n",
      "loss is :  4.64109\n",
      "loss is :  4.64006\n",
      "loss is :  4.63904\n",
      "loss is :  4.63802\n",
      "loss is :  4.637\n",
      "loss is :  4.63598\n",
      "loss is :  4.63497\n",
      "loss is :  4.63396\n",
      "loss is :  4.63294\n",
      "loss is :  4.63193\n",
      "loss is :  4.63093\n",
      "loss is :  4.62992\n",
      "loss is :  4.62892\n",
      "loss is :  4.62791\n",
      "loss is :  4.62691\n",
      "loss is :  4.62591\n",
      "loss is :  4.62491\n",
      "loss is :  4.62392\n",
      "loss is :  4.62292\n",
      "loss is :  4.62193\n",
      "loss is :  4.62094\n",
      "loss is :  4.61995\n",
      "loss is :  4.61896\n",
      "loss is :  4.61797\n",
      "loss is :  4.61699\n",
      "loss is :  4.61601\n",
      "loss is :  4.61503\n",
      "loss is :  4.61405\n",
      "loss is :  4.61307\n",
      "loss is :  4.61209\n",
      "loss is :  4.61112\n",
      "loss is :  4.61014\n",
      "loss is :  4.60917\n",
      "loss is :  4.6082\n",
      "loss is :  4.60724\n",
      "loss is :  4.60627\n",
      "loss is :  4.6053\n",
      "loss is :  4.60434\n",
      "loss is :  4.60338\n",
      "loss is :  4.60242\n",
      "loss is :  4.60146\n",
      "loss is :  4.6005\n",
      "loss is :  4.59955\n",
      "loss is :  4.59859\n",
      "loss is :  4.59764\n",
      "loss is :  4.59669\n",
      "loss is :  4.59574\n",
      "loss is :  4.59479\n",
      "loss is :  4.59385\n",
      "loss is :  4.5929\n",
      "loss is :  4.59196\n",
      "loss is :  4.59102\n",
      "loss is :  4.59008\n",
      "loss is :  4.58914\n",
      "loss is :  4.5882\n",
      "loss is :  4.58727\n",
      "loss is :  4.58633\n",
      "loss is :  4.5854\n",
      "loss is :  4.58447\n",
      "loss is :  4.58354\n",
      "loss is :  4.58261\n",
      "loss is :  4.58169\n",
      "loss is :  4.58076\n",
      "loss is :  4.57984\n",
      "loss is :  4.57892\n",
      "loss is :  4.578\n",
      "loss is :  4.57708\n",
      "loss is :  4.57616\n",
      "loss is :  4.57525\n",
      "loss is :  4.57433\n",
      "loss is :  4.57342\n",
      "loss is :  4.57251\n",
      "loss is :  4.5716\n",
      "loss is :  4.57069\n",
      "loss is :  4.56978\n",
      "loss is :  4.56888\n",
      "loss is :  4.56797\n",
      "loss is :  4.56707\n",
      "loss is :  4.56617\n",
      "loss is :  4.56527\n",
      "loss is :  4.56437\n",
      "loss is :  4.56347\n",
      "loss is :  4.56257\n",
      "loss is :  4.56168\n",
      "loss is :  4.56079\n",
      "loss is :  4.5599\n",
      "loss is :  4.559\n",
      "loss is :  4.55812\n",
      "loss is :  4.55723\n",
      "loss is :  4.55634\n",
      "loss is :  4.55546\n",
      "loss is :  4.55457\n",
      "loss is :  4.55369\n",
      "loss is :  4.55281\n",
      "loss is :  4.55193\n",
      "loss is :  4.55105\n",
      "loss is :  4.55018\n",
      "loss is :  4.5493\n",
      "loss is :  4.54843\n",
      "loss is :  4.54755\n",
      "loss is :  4.54668\n",
      "loss is :  4.54581\n",
      "loss is :  4.54495\n",
      "loss is :  4.54408\n",
      "loss is :  4.54321\n",
      "loss is :  4.54235\n",
      "loss is :  4.54148\n",
      "loss is :  4.54062\n",
      "loss is :  4.53976\n",
      "loss is :  4.5389\n",
      "loss is :  4.53804\n",
      "loss is :  4.53719\n",
      "loss is :  4.53633\n",
      "loss is :  4.53548\n",
      "loss is :  4.53463\n",
      "loss is :  4.53377\n",
      "loss is :  4.53292\n",
      "loss is :  4.53207\n",
      "loss is :  4.53123\n",
      "loss is :  4.53038\n",
      "loss is :  4.52953\n",
      "loss is :  4.52869\n",
      "loss is :  4.52785\n",
      "loss is :  4.52701\n",
      "loss is :  4.52617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  4.52533\n",
      "loss is :  4.52449\n",
      "loss is :  4.52365\n",
      "loss is :  4.52282\n",
      "loss is :  4.52198\n",
      "loss is :  4.52115\n",
      "loss is :  4.52032\n",
      "loss is :  4.51949\n",
      "loss is :  4.51866\n",
      "loss is :  4.51783\n",
      "loss is :  4.517\n",
      "loss is :  4.51618\n",
      "loss is :  4.51535\n",
      "loss is :  4.51453\n",
      "loss is :  4.51371\n",
      "loss is :  4.51289\n",
      "loss is :  4.51207\n",
      "loss is :  4.51125\n",
      "loss is :  4.51043\n",
      "loss is :  4.50962\n",
      "loss is :  4.5088\n",
      "loss is :  4.50799\n",
      "loss is :  4.50717\n",
      "loss is :  4.50636\n",
      "loss is :  4.50555\n",
      "loss is :  4.50474\n",
      "loss is :  4.50394\n",
      "loss is :  4.50313\n",
      "loss is :  4.50232\n",
      "loss is :  4.50152\n",
      "loss is :  4.50072\n",
      "loss is :  4.49991\n",
      "loss is :  4.49911\n",
      "loss is :  4.49831\n",
      "loss is :  4.49751\n",
      "loss is :  4.49672\n",
      "loss is :  4.49592\n",
      "loss is :  4.49512\n",
      "loss is :  4.49433\n",
      "loss is :  4.49354\n",
      "loss is :  4.49274\n",
      "loss is :  4.49195\n",
      "loss is :  4.49116\n",
      "loss is :  4.49037\n",
      "loss is :  4.48959\n",
      "loss is :  4.4888\n",
      "loss is :  4.48802\n",
      "loss is :  4.48723\n",
      "loss is :  4.48645\n",
      "loss is :  4.48566\n",
      "loss is :  4.48488\n",
      "loss is :  4.48411\n",
      "loss is :  4.48333\n",
      "loss is :  4.48255\n",
      "loss is :  4.48177\n",
      "loss is :  4.481\n",
      "loss is :  4.48022\n",
      "loss is :  4.47945\n",
      "loss is :  4.47867\n",
      "loss is :  4.4779\n",
      "loss is :  4.47713\n",
      "loss is :  4.47636\n",
      "loss is :  4.47559\n",
      "loss is :  4.47483\n",
      "loss is :  4.47406\n",
      "loss is :  4.4733\n",
      "loss is :  4.47253\n",
      "loss is :  4.47177\n",
      "loss is :  4.47101\n",
      "loss is :  4.47025\n",
      "loss is :  4.46949\n",
      "loss is :  4.46873\n",
      "loss is :  4.46797\n",
      "loss is :  4.46721\n",
      "loss is :  4.46645\n",
      "loss is :  4.4657\n",
      "loss is :  4.46495\n",
      "loss is :  4.46419\n",
      "loss is :  4.46344\n",
      "loss is :  4.46269\n",
      "loss is :  4.46194\n",
      "loss is :  4.46119\n",
      "loss is :  4.46044\n",
      "loss is :  4.4597\n",
      "loss is :  4.45895\n",
      "loss is :  4.45821\n",
      "loss is :  4.45746\n",
      "loss is :  4.45672\n",
      "loss is :  4.45598\n",
      "loss is :  4.45524\n",
      "loss is :  4.4545\n",
      "loss is :  4.45376\n",
      "loss is :  4.45302\n",
      "loss is :  4.45228\n",
      "loss is :  4.45155\n",
      "loss is :  4.45081\n",
      "loss is :  4.45008\n",
      "loss is :  4.44934\n",
      "loss is :  4.44861\n",
      "loss is :  4.44788\n",
      "loss is :  4.44715\n",
      "loss is :  4.44642\n",
      "loss is :  4.44569\n",
      "loss is :  4.44496\n",
      "loss is :  4.44423\n",
      "loss is :  4.44351\n",
      "loss is :  4.44278\n",
      "loss is :  4.44206\n",
      "loss is :  4.44134\n",
      "loss is :  4.44062\n",
      "loss is :  4.43989\n",
      "loss is :  4.43917\n",
      "loss is :  4.43845\n",
      "loss is :  4.43774\n",
      "loss is :  4.43702\n",
      "loss is :  4.4363\n",
      "loss is :  4.43559\n",
      "loss is :  4.43487\n",
      "loss is :  4.43416\n",
      "loss is :  4.43344\n",
      "loss is :  4.43273\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) #make sure you do this!\n",
    "\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "n_iters = 1000\n",
    "# train for n_iter iterations\n",
    "\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"/home/madhusudan/environments/tensorboard/wordtovec/word.ckpt\")\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = 'word2vec'\n",
    "embedding.metadata_path = \"/home/madhusudan/environments/tensorboard/wordtovec/metadata.tsv\"\n",
    "writer = tf.summary.FileWriter(\"/home/madhusudan/environments/tensorboard/wordtovec/my_graph\",sess.graph)\n",
    "projector.visualize_embeddings(tf.summary.FileWriter(\"/home/madhusudan/environments/tensorboard/wordtovec/\"), config)\n",
    "print (\"Done\")\n",
    "\n",
    "vectors = sess.run(W1 + b1)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.78933942 -0.61395707]\n",
      " [ 0.62731192  0.7787681 ]\n",
      " [-0.97844257 -0.20651908]\n",
      " [-0.99764292  0.06861924]\n",
      " [-0.9235113   0.38357122]\n",
      " [-0.26561799 -0.96407836]\n",
      " [-0.07030869  0.99752528]\n",
      " [ 0.90539427 -0.4245718 ]\n",
      " [ 0.81394757  0.58093834]\n",
      " [ 0.86886499  0.49504911]\n",
      " [-0.90413634 -0.42724406]\n",
      " [ 0.54650522 -0.83745569]\n",
      " [ 0.67488073  0.73792682]\n",
      " [ 0.29715878  0.95482808]\n",
      " [ 0.97972813 -0.20033173]\n",
      " [-0.97116803 -0.23839602]\n",
      " [-0.95236453  0.30496196]\n",
      " [-0.93421668 -0.35670603]\n",
      " [-0.94114005  0.33801687]\n",
      " [-0.98954249  0.14424167]\n",
      " [ 0.9922252  -0.12445539]\n",
      " [ 0.38225966 -0.92405495]\n",
      " [ 0.99985833  0.01683225]\n",
      " [-0.68244432 -0.73093758]\n",
      " [ 0.57398389 -0.81886659]\n",
      " [-0.88130038 -0.4725565 ]\n",
      " [ 0.9540534  -0.29963662]\n",
      " [-0.51857655  0.85503121]\n",
      " [-0.69045351 -0.72337677]\n",
      " [-0.0062009   0.99998077]\n",
      " [ 0.94926038 -0.31449122]\n",
      " [ 0.36157777 -0.93234195]\n",
      " [-0.04301149 -0.99907458]\n",
      " [-0.17841049 -0.98395615]\n",
      " [-0.49646216  0.86805836]\n",
      " [-0.67267574 -0.73993739]\n",
      " [-0.96632674 -0.25731815]\n",
      " [-0.93478297 -0.35521938]\n",
      " [ 0.51463462  0.85740959]\n",
      " [-0.84824607 -0.5296023 ]\n",
      " [-0.18991512  0.98180051]\n",
      " [-0.06730409  0.99773251]\n",
      " [ 0.07719034  0.99701637]\n",
      " [ 0.86790498  0.49673026]\n",
      " [ 0.99968942  0.02492121]\n",
      " [-0.92250899  0.38597561]\n",
      " [ 0.95313974  0.3025304 ]\n",
      " [ 0.9990555   0.04345228]\n",
      " [-0.82853833  0.55993235]\n",
      " [-0.86285825 -0.50544598]\n",
      " [ 0.51435176 -0.85757931]\n",
      " [-0.61611396  0.78765702]\n",
      " [ 0.79783145 -0.60288056]\n",
      " [-0.341943    0.93972069]\n",
      " [ 0.99451068 -0.10463515]\n",
      " [-0.90397401 -0.42758741]\n",
      " [-0.59688776  0.80232475]\n",
      " [-0.98506332  0.1721925 ]\n",
      " [ 0.31110793  0.95037459]\n",
      " [ 0.96981834  0.24382861]\n",
      " [-0.34310039 -0.93929874]\n",
      " [-0.71484609  0.69928183]\n",
      " [ 0.85632316 -0.51644036]\n",
      " [ 0.91832306 -0.39583172]\n",
      " [-0.52671448  0.85004227]\n",
      " [ 0.97088223  0.23955727]\n",
      " [ 0.81632     0.57759992]\n",
      " [ 0.74869558 -0.66291397]\n",
      " [ 0.14421766  0.98954599]\n",
      " [ 0.34258606 -0.93948645]\n",
      " [-0.42189529  0.90664456]\n",
      " [ 0.53298701 -0.84612342]\n",
      " [ 0.7207299  -0.693216  ]\n",
      " [ 0.86813955  0.49632018]\n",
      " [-0.54591671 -0.83783945]\n",
      " [-0.9491202   0.31491402]\n",
      " [ 0.07149688 -0.99744082]\n",
      " [-0.99990308  0.013922  ]\n",
      " [-0.44334523 -0.89635094]\n",
      " [ 0.71665897  0.69742377]\n",
      " [ 0.97847653  0.20635815]\n",
      " [-0.99852939  0.05421309]\n",
      " [ 0.52306145 -0.85229497]\n",
      " [ 0.31418509 -0.94936175]\n",
      " [ 0.47530483  0.87982118]\n",
      " [ 0.92632383 -0.37672824]\n",
      " [-0.35577018  0.93457347]\n",
      " [-0.2502026   0.9681935 ]\n",
      " [ 0.99675286 -0.08052163]\n",
      " [-0.89216741  0.4517049 ]\n",
      " [ 0.83631722 -0.54824584]\n",
      " [ 0.1290415  -0.99163919]\n",
      " [ 0.71247407 -0.70169844]\n",
      " [-0.80864216  0.58830082]\n",
      " [ 0.99469983  0.10282148]\n",
      " [ 0.65320374  0.75718219]\n",
      " [-0.75883384 -0.65128428]\n",
      " [ 0.79886839  0.60150585]\n",
      " [ 0.35374864  0.93534053]\n",
      " [ 0.78498268 -0.61951772]\n",
      " [-0.98976937 -0.14267656]\n",
      " [ 0.83472154 -0.55067226]\n",
      " [ 0.70189658 -0.71227888]\n",
      " [-0.44121247 -0.89740267]\n",
      " [-0.60998438 -0.79241344]\n",
      " [-0.99997537 -0.00701787]\n",
      " [ 0.12322557 -0.99237869]\n",
      " [-0.80353937  0.59525161]\n",
      " [-0.87312347  0.48749914]\n",
      " [ 0.16083452 -0.98698139]\n",
      " [ 0.74514072 -0.66690727]\n",
      " [ 0.56031005  0.82828295]\n",
      " [-0.94007755  0.34096069]\n",
      " [-0.69799953  0.71609822]\n",
      " [-0.91345446  0.40694096]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors) \n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')\n",
    "\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'machine', 'demand', 'much', 'as', 'market', 'each', 'purpose', 'it', 'widely', 'success', 'from', 'for', 'perfect', 'according', 'behind', 'an', 'between', 'its', 'understand', 'with', 'time', 'to', 'productive', 'flexible', 'these', 'which', 'can', 'flow', 'major', 'operations', 'but', 'more', 'it’s', 'there', 'make', 'smartphones', 'through', 'will', 'that', 'computation', 'numeric', 'technology', 'production', 'run', 'you', 'on', 'along', 'so', 'has', 'ranging', 'be', 'computations', 'a', 'google’s', 'great', 'open', 'tensorboard', 'could', 'library', 'right', 'program', 'one', 'definition', 'graph', 'the', 'community', 'now', 'data', 'addition', 'simple', 'research', 'comes', 'node', 'your', 'even', 'tasks', 'in', 'nodes', 'was', 'documentation', 'accessed', 'developed', 'brain', 'diagrams', 'general', 'know', 'represents', 'tensorflow', 'is', 'being', 'enthusiast', 'team', 'reason', 'our', 'api', 'represent', 'used', 'of', 'specific', 'form', 'performing', 'source', 'relation', 'ready', 'very', 'by', 'are', 'if', 'nature', 'put', 'learning', 'active', 'edge', 'bag'}\n",
      "and 2.22419\n",
      "machine 0.991183\n",
      "demand 0.290617\n",
      "much -0.729141\n",
      "as -0.273283\n",
      "market -0.475345\n",
      "each 2.9861\n",
      "purpose 0.928021\n",
      "it 0.3031\n",
      "widely 0.289777\n",
      "success 0.13837\n",
      "from -1.03546\n",
      "for 0.87118\n",
      "perfect -1.58742\n",
      "according -1.12751\n",
      "behind 0.45448\n",
      "an -0.654944\n",
      "between -0.0469306\n",
      "its 0.9541\n",
      "understand -0.14065\n",
      "with -3.08666\n",
      "time -1.75958\n",
      "to -2.69977\n",
      "productive 0.640301\n",
      "flexible 0.202441\n",
      "these 0.338305\n",
      "which -0.672825\n",
      "can 0.97851\n",
      "flow -1.77253\n",
      "major -0.21958\n",
      "operations 1.49129\n",
      "but -0.806058\n",
      "more -1.74012\n",
      "it’s -0.77985\n",
      "there 0.83194\n",
      "make 1.30607\n",
      "smartphones -0.328172\n",
      "through -0.553705\n",
      "will 1.18163\n",
      "that 0.175923\n",
      "computation 0.102149\n",
      "numeric 0.641362\n",
      "technology -0.731792\n",
      "production -0.786811\n",
      "run -0.268663\n",
      "you 0.554996\n",
      "on -0.300423\n",
      "along -2.71213\n",
      "so 0.0699273\n",
      "has 0.89843\n",
      "ranging -0.919304\n",
      "be 0.897273\n",
      "computations 2.08236\n",
      "a 2.30473\n",
      "google’s -1.17906\n",
      "great 0.485275\n",
      "open -0.203613\n",
      "tensorboard -1.02259\n",
      "could 0.419885\n",
      "library 0.601417\n",
      "right 1.43828\n",
      "program 0.138644\n",
      "one -2.20993\n",
      "definition -2.52894\n",
      "graph 1.81601\n",
      "the -0.9227\n",
      "community 0.639098\n",
      "now -0.797329\n",
      "data -2.43032\n",
      "addition 0.909984\n",
      "simple 0.70335\n",
      "research -0.645641\n",
      "comes -1.63072\n",
      "node 2.25347\n",
      "your -1.233\n",
      "even 0.213563\n",
      "tasks 0.911584\n",
      "in -0.823948\n",
      "nodes -0.0972703\n",
      "was -1.71092\n",
      "documentation -1.35704\n",
      "accessed -0.296351\n",
      "developed -1.4819\n",
      "brain -0.0543065\n",
      "diagrams -2.99352\n",
      "general 1.7289\n",
      "know 1.34056\n",
      "represents 2.8913\n",
      "tensorflow -1.33537\n",
      "is -0.644471\n",
      "being 0.411896\n",
      "enthusiast 0.139076\n",
      "team 0.208726\n",
      "reason -0.526197\n",
      "our -1.51413\n",
      "api -1.58349\n",
      "represent 1.52588\n",
      "used 0.154664\n",
      "of -2.24448\n",
      "specific -2.62819\n",
      "form -1.2565\n",
      "performing 1.08985\n",
      "source 0.571524\n",
      "relation 1.14392\n",
      "ready 1.67209\n",
      "very 0.128081\n",
      "by -1.70868\n",
      "are 1.95126\n",
      "if 0.959001\n",
      "nature -0.0779479\n",
      "put -1.66468\n",
      "learning 0.978278\n",
      "active -0.233962\n",
      "edge 1.14844\n",
      "bag -0.456498\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEUCAYAAADwYOuyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQodERFxjEJHREQc8/8BkwcPRcAD2a8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f207087c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "print(words)\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
